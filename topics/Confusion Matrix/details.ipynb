{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb630df",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25612baf",
   "metadata": {},
   "source": [
    "- Karl Pearson created the confusion matrix in 1904, when it was first known as a contingency table. It was later referred to as a classification matrix, before being referred to as a confusion matrix in data science.\n",
    "- The word \"confusion\" refers to confusion that can occur on a specific metric to be prioritized while attempting to improve the model, although several metrics can be obtained from the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a648a0",
   "metadata": {},
   "source": [
    "The **confusion matrix** is a square matrix of size *N √ó N*, where N denotes the number of output classes.\n",
    "Each row of the matrix represents the number of instances of a predicted class and each column represents the number of instances of the actual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55343bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------------------+------------------------------------+\n",
      "|                 | Predicted Positive                | Predicted Negative                 |\n",
      "+=================+===================================+====================================+\n",
      "| Actual Positive | True Positive (TP)                | False Negative (Type II error, FN) |\n",
      "+-----------------+-----------------------------------+------------------------------------+\n",
      "| Actual Negative | False Positive (Type I error, FP) | True Negative (TN)                 |\n",
      "+-----------------+-----------------------------------+------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "confusion_matrix = [\n",
    "    [\"\", \"Predicted Positive\", \"Predicted Negative\"],\n",
    "    [\"Actual Positive\", \"True Positive (TP)\", \"False Negative (Type II error, FN)\"],\n",
    "    [\"Actual Negative\", \"False Positive (Type I error, FP)\", \"True Negative (TN)\"]\n",
    "]\n",
    "\n",
    "print(tabulate(confusion_matrix, headers=\"firstrow\", tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249e282",
   "metadata": {},
   "source": [
    "There are two types of prediction: correct and incorrect (errors). \n",
    "* **True Positive (TP)**: Both the actual values and the prediction are positive. \n",
    "* **False Positive (FP)**: Although the prediction is positive, the actual value is negative. This is called a \"Type I error‚Äù.\n",
    "* **True Negative (TN)**: The actual value is negative, and the prediction is negative.\n",
    "* **False Negative (FN)**: Although predicted to be negative, the sample is positive. This is also called \"Type II error‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990faa2",
   "metadata": {},
   "source": [
    "### üìä Metrics Derived from the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be269d",
   "metadata": {},
   "source": [
    "The confusion matrix is not only a table but also the foundation for a whole family of evaluation metrics and visualizations in classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d0279",
   "metadata": {},
   "source": [
    "#### Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec0137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Error\\ Rate = \\frac{FP + FN}{TP + TN + FP + FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "formulas = [\n",
    "    r\"Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\",\n",
    "    r\"Error\\ Rate = \\frac{FP + FN}{TP + TN + FP + FN}\"\n",
    "]\n",
    "\n",
    "for f in formulas:\n",
    "    display(Math(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd679143",
   "metadata": {},
   "source": [
    "#### Class-wise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96f3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Precision \\ (Positive \\ Predictive \\ Value) = \\frac{TP}{TP + FP}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Recall \\ (Sensitivity, \\ True \\ Positive \\ Rate) = \\frac{TP}{TP + FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Specificity \\ (True \\ Negative \\ Rate) = \\frac{TN}{TN + FP}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle False \\ Positive \\ Rate \\ (FPR) = \\frac{FP}{FP + TN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle False \\ Negative \\ Rate \\ (FNR) = \\frac{FN}{FN + TP}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "formulas = [\n",
    "    r\"Precision \\ (Positive \\ Predictive \\ Value) = \\frac{TP}{TP + FP}\",\n",
    "    r\"Recall \\ (Sensitivity, \\ True \\ Positive \\ Rate) = \\frac{TP}{TP + FN}\",\n",
    "    r\"Specificity \\ (True \\ Negative \\ Rate) = \\frac{TN}{TN + FP}\",\n",
    "    r\"False \\ Positive \\ Rate \\ (FPR) = \\frac{FP}{FP + TN}\",\n",
    "    r\"False \\ Negative \\ Rate \\ (FNR) = \\frac{FN}{FN + TP}\"\n",
    "]\n",
    "\n",
    "for f in formulas:\n",
    "    display(Math(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc8e9b",
   "metadata": {},
   "source": [
    "#### Balanced Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad3a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle F1\\text{-}Score = \\frac{2 \\times (Precision \\times Recall)}{Precision + Recall}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Balanced\\ Accuracy = \\frac{Sensitivity + Specificity}{2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G\\text{-}Mean = \\sqrt{Sensitivity \\times Specificity}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Matthews Correlation Coefficient (MCC) = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "formulas = [\n",
    "    # F1 Score\n",
    "    r\"F1\\text{-}Score = \\frac{2 \\times (Precision \\times Recall)}{Precision + Recall}\",\n",
    "    \n",
    "    # Balanced Accuracy\n",
    "    r\"Balanced\\ Accuracy = \\frac{Sensitivity + Specificity}{2}\",\n",
    "    \n",
    "    # G-Mean\n",
    "    r\"G\\text{-}Mean = \\sqrt{Sensitivity \\times Specificity}\",\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    r\"Matthews Correlation Coefficient (MCC) = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\"\n",
    "]\n",
    "\n",
    "for f in formulas:\n",
    "    display(Math(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c05d8",
   "metadata": {},
   "source": [
    "#### Probabilistic Metrics (with threshold variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a2c05",
   "metadata": {},
   "source": [
    "* ROC-AUC (Receiver Operating Characteristic ‚Äì Area Under Curve)\n",
    "    - Plots TPR (Recall) vs FPR at different thresholds.\n",
    "    - Area under the curve = model‚Äôs discrimination ability.\n",
    "* PR-AUC (Precision-Recall Curve ‚Äì Area Under Curve)\n",
    "    - Plots Precision vs Recall at different thresholds.\n",
    "    - Useful for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7485446",
   "metadata": {},
   "source": [
    "#### Chance-Corrected Agreement Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab175633",
   "metadata": {},
   "source": [
    "* Cohen‚Äôs Kappa\n",
    "* Informedness (Bookmaker Informedness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac44dd",
   "metadata": {},
   "source": [
    "#### Data Distribution Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1aa43",
   "metadata": {},
   "source": [
    "* Prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c17c92",
   "metadata": {},
   "source": [
    "#### Predictive Association Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604a6de",
   "metadata": {},
   "source": [
    "* Markedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648973b",
   "metadata": {},
   "source": [
    "#### Baseline / Benchmark Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc792961",
   "metadata": {},
   "source": [
    "* Null Error Rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
